{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/banupriya11/ai-agents?scriptVersionId=225484679\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AI Agents for Engineers (Evolution of AI Agents)\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/workshops/ai-agents/ai_agents_for_engineers.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>","metadata":{"id":"JAPoU8Sm5E6e"}},{"cell_type":"markdown","source":"| | |\n|-|-|\n| Author(s) | [Kristopher Overholt](https://github.com/koverholt) [Holt Skinner](https://github.com/holtskinner)|","metadata":{"id":"84f0f73a0f76"}},{"cell_type":"markdown","source":"## Overview\n\nThis notebook demonstrates 3 different approaches to generating essays using the [Gemini Developer API](https://ai.google.dev/gemini-api/docs) or [Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview). Each method illustrates a distinct paradigm for running AI Agents in differing levels of complexity.\n\n1. Zero-Shot Approach with the Gemini API\n2. Step-by-Step Approach With LangChain\n3. Iterative, AI-Agent Approach with LangGraph","metadata":{"id":"tvgnzT1CKxrO"}},{"cell_type":"markdown","source":"## Get started","metadata":{"id":"61RBz8LLbxCR"}},{"cell_type":"markdown","source":"### Install Gemini SDK and other required packages\n","metadata":{"id":"No17Cw5hgx12"}},{"cell_type":"code","source":"%pip install --upgrade --quiet \\\n    google-genai \\\n    langgraph \\\n    langchain \\\n    langchain-google-genai \\\n    langchain-google-vertexai \\\n    langchain-community \\\n    tavily-python \\\n    pydantic","metadata":{"id":"tFy3H3aPgx12","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:49:20.754569Z","iopub.execute_input":"2025-03-03T10:49:20.754956Z","iopub.status.idle":"2025-03-03T10:49:26.956386Z","shell.execute_reply.started":"2025-03-03T10:49:20.754929Z","shell.execute_reply":"2025-03-03T10:49:26.954996Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Restart runtime\n\nTo use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n\nThe restart might take a minute or longer. After it's restarted, continue to the next step.","metadata":{"id":"R5Xep4W9lq-Z"}},{"cell_type":"code","source":"import IPython\n\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)","metadata":{"id":"XRvKdaPDTznN","outputId":"af4192bf-e0ef-4a9f-89d6-5cd27d7d6103","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:49:35.545889Z","iopub.execute_input":"2025-03-03T10:49:35.546248Z","iopub.status.idle":"2025-03-03T10:49:35.553359Z","shell.execute_reply.started":"2025-03-03T10:49:35.546218Z","shell.execute_reply":"2025-03-03T10:49:35.552411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">\n<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n</div>\n","metadata":{"id":"SbmM4z7FOBpM"}},{"cell_type":"markdown","source":"### Configure Tavily\n\nGet an API key for [Tavily](https://tavily.com/), a web search API for Generative AI models.","metadata":{"id":"b1824bd79df4"}},{"cell_type":"code","source":"import os\n\nos.environ[\"TAVILY_API_KEY\"] = \"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\"","metadata":{"id":"5df2f86c691f","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:49:44.090043Z","iopub.execute_input":"2025-03-03T10:49:44.090423Z","iopub.status.idle":"2025-03-03T10:49:44.094998Z","shell.execute_reply.started":"2025-03-03T10:49:44.090394Z","shell.execute_reply":"2025-03-03T10:49:44.093622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If your API Keys are in Colab Secrets\n# import sys\n\n# if \"google.colab\" in sys.modules:\n#     from google.colab import userdata\n\n#     os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")","metadata":{"id":"e00844b64cf7"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Gemini Developer API\n\nGet API keys from [Google AI Studio](https://ai.google.dev/gemini-api/docs/api-key) and [Tavily](https://tavily.com/).","metadata":{"id":"0781fd4c9001"}},{"cell_type":"code","source":"import os\n\nos.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAezEhJ0OxrkWR35yadIgpw5WDLek2WcG0\"","metadata":{"id":"f2d2ecd0e96d","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:49:48.464781Z","iopub.execute_input":"2025-03-03T10:49:48.465134Z","iopub.status.idle":"2025-03-03T10:49:48.469398Z","shell.execute_reply.started":"2025-03-03T10:49:48.465105Z","shell.execute_reply":"2025-03-03T10:49:48.468256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If your API Keys are in Colab Secrets\n\n\n# from google.colab import userdata\n\n# os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")","metadata":{"id":"nvuDx1V6buOQ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Vertex AI\n\n**Use a Google Cloud Project:** This requires enabling the Vertex AI API in your Google Cloud project.\n\n[Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)","metadata":{"id":"5ffb0f859dde"}},{"cell_type":"code","source":"# import sys\n\n# if \"google.colab\" in sys.modules:\n#     from google.colab import auth\n\n#     auth.authenticate_user()","metadata":{"id":"885a3c84ddac"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PROJECT_ID = \"1\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n# if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n#     PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n\n# LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n\n# os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"true\"","metadata":{"id":"c80118166880"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generating Essays Using a Zero-Shot Approach with the Gemini API\n\nWith just a single call to the `generate_content` method, users can create detailed, structured essays on any topic by leveraging state-of-the-art language models such as Gemini 1.5 Pro or Gemini 1.5 Flash.\n\n<img src=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/workshops/ai-agents/1-prompt-essay.png?raw=1\" width=\"350px\">","metadata":{"id":"EdvJRUWRNGHE"}},{"cell_type":"markdown","source":"### Import libraries","metadata":{"id":"5303c05f7aa6"}},{"cell_type":"code","source":"from IPython.display import Markdown, display","metadata":{"id":"6fc324893334","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:10.15548Z","iopub.execute_input":"2025-03-03T10:50:10.155955Z","iopub.status.idle":"2025-03-03T10:50:10.161451Z","shell.execute_reply.started":"2025-03-03T10:50:10.155915Z","shell.execute_reply":"2025-03-03T10:50:10.160191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Client","metadata":{"id":"4631829d00c8"}},{"cell_type":"code","source":"from google import genai\n\nclient = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])","metadata":{"id":"898c62c59d40","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:49:53.802566Z","iopub.execute_input":"2025-03-03T10:49:53.802923Z","iopub.status.idle":"2025-03-03T10:49:55.332803Z","shell.execute_reply.started":"2025-03-03T10:49:53.802897Z","shell.execute_reply":"2025-03-03T10:49:55.33173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Verify which API you are using.","metadata":{"id":"4b87f1f593d5"}},{"cell_type":"code","source":"# if not client.vertexai:\n#     print(f\"Using Gemini Developer API.\")\n# elif client._api_client.project:\n#     print(\n#         f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n#     )\n# elif client._api_client.api_key:\n#     print(\n#         f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n#     )","metadata":{"id":"84c16a5f3f33","outputId":"aab12b78-63df-45ed-ce60-941ca0a42859"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load model","metadata":{"id":"e43229f3ad4f"}},{"cell_type":"code","source":"MODEL_ID = \"gemini-2.0-flash-001\"","metadata":{"id":"cf93d5f0ce00","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:49:58.202093Z","iopub.execute_input":"2025-03-03T10:49:58.202686Z","iopub.status.idle":"2025-03-03T10:49:58.207099Z","shell.execute_reply.started":"2025-03-03T10:49:58.202652Z","shell.execute_reply":"2025-03-03T10:49:58.205982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Make an API call to generate the essay","metadata":{"id":"bbcd4f57190f"}},{"cell_type":"code","source":"prompt = \"Write a 3-paragraph essay about the application of heat transfer in modern data centers\"\n\nresponse = client.models.generate_content(model=MODEL_ID, contents=prompt)\ndisplay(Markdown(response.text))","metadata":{"id":"3734f520c1b3","outputId":"9d031b14-f6f8-4d51-d04b-ea6d03c7f36c","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:13.724318Z","iopub.execute_input":"2025-03-03T10:50:13.724747Z","iopub.status.idle":"2025-03-03T10:50:17.029084Z","shell.execute_reply.started":"2025-03-03T10:50:13.724714Z","shell.execute_reply":"2025-03-03T10:50:17.02788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\nHowever, what if we ask the model to write an essay about an event that happened more recently and the LLM doesn't inherently know about that event?","metadata":{"id":"520e23ea4332"}},{"cell_type":"code","source":"prompt = \"Write a 3-paragraph essay about the impacts of Hurricane Helene and Hurricane Milton in 2024.\"\nresponse = client.models.generate_content(model=MODEL_ID, contents=prompt)\ndisplay(Markdown(response.text))","metadata":{"id":"dcb775b83997","outputId":"e0e21cee-38b5-42a1-826e-bb097186cb61","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:17.880453Z","iopub.execute_input":"2025-03-03T10:50:17.880818Z","iopub.status.idle":"2025-03-03T10:50:22.064496Z","shell.execute_reply.started":"2025-03-03T10:50:17.88079Z","shell.execute_reply":"2025-03-03T10:50:22.063369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this case, the model had no information about these recent events and was unable to write an effective essay.","metadata":{"id":"764ce71aecd5"}},{"cell_type":"markdown","source":"## Generating Essays Using a Step-by-Step Approach With LangChain\n\nThis step demonstrates how to build an essay-writing pipeline using [LangChain](https://www.langchain.com/), the [Gemini API in Google AI Studio](https://ai.google.dev/gemini-api/docs), and [Tavily](https://tavily.com/) for search.\n\nBy combining these tools, we create a seamless workflow that plans an essay outline, performs web searches for relevant information, and generates a complete essay draft based on the collected data.\n\nThis solution showcases the power of chaining LLM models and external tools to tackle complex tasks with minimal human intervention, providing a robust approach to automated content generation.\n\n<img src=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/workshops/ai-agents/2-langchain-essay.png?raw=1\" width=\"550px\">\n","metadata":{"id":"16aafc60d80b"}},{"cell_type":"markdown","source":"### Import libraries","metadata":{"id":"85666976a359"}},{"cell_type":"code","source":"from IPython.display import Markdown, display\nfrom langchain import LLMChain\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_community.tools import TavilySearchResults\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_google_vertexai import ChatVertexAI","metadata":{"id":"29d6e42d27ae","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:22.065784Z","iopub.execute_input":"2025-03-03T10:50:22.066172Z","iopub.status.idle":"2025-03-03T10:50:35.116639Z","shell.execute_reply.started":"2025-03-03T10:50:22.066131Z","shell.execute_reply":"2025-03-03T10:50:35.115658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initialize Gemini model & search tool","metadata":{"id":"43392590b1d8"}},{"cell_type":"code","source":"\nmodel = ChatGoogleGenerativeAI(model=MODEL_ID, temperature=0)\ntavily_tool = TavilySearchResults(max_results=5)","metadata":{"id":"6f8b0c205551","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:35.118613Z","iopub.execute_input":"2025-03-03T10:50:35.119342Z","iopub.status.idle":"2025-03-03T10:50:35.199345Z","shell.execute_reply.started":"2025-03-03T10:50:35.119294Z","shell.execute_reply":"2025-03-03T10:50:35.198451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define prompt templates and Runnables","metadata":{"id":"7ee8707e1867"}},{"cell_type":"code","source":"# Planning: Create an outline for the essay\noutline_template = ChatPromptTemplate.from_template(\n    \"Create a detailed outline for an essay on {topic}\"\n)\n\n\n# Research: Web search\ndef research_fn(topic):\n    response = tavily_tool.invoke({\"query\": topic})\n    return \"\\n\".join([f\"- {result['content']}\" for result in response])\n\n\n# Writing: Write the essay based on outline and research\nwriting_template = ChatPromptTemplate.from_template(\n    \"Based on the following outline and research, write a 3-paragraph essay on '{topic}':\\n\\nOutline:\\n{outline}\\n\\nResearch:\\n{research}\\n\\nEssay:\"\n)","metadata":{"id":"a09a6a6d1f36","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:35.200475Z","iopub.execute_input":"2025-03-03T10:50:35.200745Z","iopub.status.idle":"2025-03-03T10:50:35.206512Z","shell.execute_reply.started":"2025-03-03T10:50:35.200723Z","shell.execute_reply":"2025-03-03T10:50:35.205376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define the Runnable Chain using [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel)","metadata":{"id":"9a18006523f7"}},{"cell_type":"code","source":"# Define individual chains\noutline_chain = LLMChain(llm=model, prompt=outline_template)\nwriting_chain = LLMChain(llm=model, prompt=writing_template)\n\n# Use the pipe operator to combine chains\nchain = (\n    outline_chain\n    | (\n        lambda result: {\n            \"topic\": result[\"topic\"],\n            \"outline\": result[\"text\"],\n            \"research\": research_fn(result[\"topic\"]),\n        }\n    )\n    | writing_chain\n    | (lambda result: result[\"text\"])  # Extract the essay text from the final result\n    | StrOutputParser()\n)","metadata":{"id":"cf48601613fd","outputId":"50bc525a-9982-4a15-d3fe-4c94d73c1d59","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:35.207536Z","iopub.execute_input":"2025-03-03T10:50:35.20783Z","iopub.status.idle":"2025-03-03T10:50:35.232088Z","shell.execute_reply.started":"2025-03-03T10:50:35.207806Z","shell.execute_reply":"2025-03-03T10:50:35.230946Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generate the essay","metadata":{"id":"839fc48dd408"}},{"cell_type":"code","source":"essay = chain.invoke({\"topic\": prompt})\ndisplay(Markdown(essay))","metadata":{"id":"a76f80ceec98","outputId":"b6bef306-e3e7-4b8f-97eb-3e3a1255ea44","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:40.393476Z","iopub.execute_input":"2025-03-03T10:50:40.393832Z","iopub.status.idle":"2025-03-03T10:50:57.408121Z","shell.execute_reply.started":"2025-03-03T10:50:40.393808Z","shell.execute_reply":"2025-03-03T10:50:57.406928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generating Essays Using an Iterative, AI-Agent Approach with LangGraph\n\nThis section demonstrates how to build a [LangGraph](https://langchain-ai.github.io/langgraph/)-powered AI agent to generate, revise, and critique essays using large language models such as Google's [Gemini API in Google AI Studio](https://ai.google.dev/gemini-api/docs) or the [Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview). The LangGraph code was adapted from the awesome DeepLearning.AI course on [AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/).\n\nBy defining a structured state flow with nodes such as \"Planner,\" \"Research Plan,\" \"Generate,\" \"Reflect,\" and \"Research Critique,\" the system iteratively creates an essay on a given topic, incorporates feedback, and provides research-backed insights.\n\n<img src=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/workshops/ai-agents/3-langgraph-essay.png?raw=1\" width=\"900px\">\n\nThe workflow enables automated essay generation with revision controls, making it ideal for structured writing tasks or educational use cases. Additionally, the notebook uses external search tools to gather and integrate real-time information into the essay content.","metadata":{"id":"294d3b7c43b2"}},{"cell_type":"markdown","source":"### Import libraries","metadata":{"id":"f8e41763086f"}},{"cell_type":"code","source":"from typing import TypedDict\n\n# Common libraries\nfrom IPython.display import Image, Markdown, display\n\n# LangChain and LangGraph components\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n# LangChain integrations for Gemini API in Google AI Studio and Vertex AI\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, StateGraph\n\n# Typing utilities for data validation and schema definitions\nfrom pydantic.v1 import BaseModel\n\n# Tavily client for performing web searches\nfrom tavily import TavilyClient","metadata":{"id":"52fbe2cb7be7","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:57.409454Z","iopub.execute_input":"2025-03-03T10:50:57.409751Z","iopub.status.idle":"2025-03-03T10:50:57.510778Z","shell.execute_reply.started":"2025-03-03T10:50:57.409711Z","shell.execute_reply":"2025-03-03T10:50:57.509908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initialize agent memory, agent state, and schema for search queries","metadata":{"id":"fc6ae1fac44f"}},{"cell_type":"code","source":"# Initialize agent memory\nmemory = MemorySaver()\n\n\n# Define the agent's state\nclass AgentState(TypedDict):#shared global state -- how your nodes talk to each other\n    task: str\n    plan: str\n    draft: str\n    critique: str\n    content: list[str]\n    revision_number: int\n    max_revisions: int\n\n\n# Define a schema for search queries\nclass Queries(BaseModel):\n    \"\"\"Variants of query to search for\"\"\"\n\n    queries: list[str]","metadata":{"id":"6b92f7bab46d","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:57.512363Z","iopub.execute_input":"2025-03-03T10:50:57.51275Z","iopub.status.idle":"2025-03-03T10:50:57.519489Z","shell.execute_reply.started":"2025-03-03T10:50:57.512713Z","shell.execute_reply":"2025-03-03T10:50:57.518117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initialize Gemini model and search tool\n\nRemember to set the environment variables `GOOGLE_API_KEY` and `TAVILY_API_KEY`. And configure credentials for Vertex AI if you switch to it.","metadata":{"id":"d9660e58afab"}},{"cell_type":"code","source":"\n# model = ChatVertexAI(model=MODEL_ID, temperature=0)\nmodel = ChatGoogleGenerativeAI(model=MODEL_ID, temperature=0)\n\n# Initialize Tavily client for performing web searches\ntavily = TavilyClient()","metadata":{"id":"ec96b00bb67f","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:57.520637Z","iopub.execute_input":"2025-03-03T10:50:57.520977Z","iopub.status.idle":"2025-03-03T10:50:57.543023Z","shell.execute_reply.started":"2025-03-03T10:50:57.520952Z","shell.execute_reply":"2025-03-03T10:50:57.541721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define prompt templates for each stage","metadata":{"id":"d94dc64d3846"}},{"cell_type":"code","source":"PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay.\nWrite such an outline for the user provided topic. Give an outline of the essay along with any\nrelevant notes or instructions for the sections.\"\"\"\n\nWRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 3-paragraph essays.\nGenerate the best essay possible for the user's request and the initial outline.\nIf the user provides critique, respond with a revised version of your previous attempts.\nUse Markdown formatting to specify a title and section headers for each paragraph.\nUtilize all of the information below as needed:\n---\n{content}\"\"\"\n\nREFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission.\nGenerate critique and recommendations for the user's submission.\nProvide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n\nRESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can\nbe used when writing the following essay. Generate a list of search queries that will gather\nany relevant information. Only generate 3 queries max.\"\"\"\n\nRESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can\nbe used when making any requested revisions (as outlined below).\nGenerate a list of search queries that will gather any relevant information.\nOnly generate 3 queries max.\"\"\"","metadata":{"id":"2cc6f9b05d29","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:50:57.577909Z","iopub.execute_input":"2025-03-03T10:50:57.578239Z","iopub.status.idle":"2025-03-03T10:50:57.583232Z","shell.execute_reply.started":"2025-03-03T10:50:57.578214Z","shell.execute_reply":"2025-03-03T10:50:57.582034Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define node functions for each stage","metadata":{"id":"c4f51c668222"}},{"cell_type":"code","source":"# Generate an outline for the essay\n\n\ndef plan_node(state: AgentState):\n    messages = [SystemMessage(content=PLAN_PROMPT), HumanMessage(content=state[\"task\"])]\n    response = model.invoke(messages)\n    return {\"plan\": response.content}\n\n\n# Conducts research based on the generated plan and web search results\ndef research_plan_node(state: AgentState):\n    queries = model.with_structured_output(Queries).invoke(\n        [\n            SystemMessage(content=RESEARCH_PLAN_PROMPT),\n            HumanMessage(content=state[\"task\"]),\n        ]\n    )\n    content = state[\"content\"] or []\n    for q in queries.queries:\n        response = tavily.search(query=q, max_results=2)\n        for r in response[\"results\"]:\n            content.append(r[\"content\"])\n    return {\"content\": content}\n\n\n# Generates a draft based on the content and plan\ndef generation_node(state: AgentState):\n    content = \"\\n\\n\".join(state[\"content\"] or [])\n    user_message = HumanMessage(\n        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\"\n    )\n    messages = [\n        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n        user_message,\n    ]\n    response = model.invoke(messages)\n    return {\n        \"draft\": response.content,\n        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n    }\n\n\n# Provides feedback or critique on the draft\ndef reflection_node(state: AgentState):\n    messages = [\n        SystemMessage(content=REFLECTION_PROMPT),\n        HumanMessage(content=state[\"draft\"]),\n    ]\n    response = model.invoke(messages)\n    return {\"critique\": response.content}\n\n\n# Conducts research based on the critique\ndef research_critique_node(state: AgentState):\n    queries = model.with_structured_output(Queries).invoke(\n        [\n            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n            HumanMessage(content=state[\"critique\"]),\n        ]\n    )\n    content = state[\"content\"] or []\n    for q in queries.queries:\n        response = tavily.search(query=q, max_results=2)\n        for r in response[\"results\"]:\n            content.append(r[\"content\"])\n    return {\"content\": content}\n\n\n# Determines whether the critique and research cycle should\n# continue based on the number of revisions\ndef should_continue(state):\n    if state[\"revision_number\"] > state[\"max_revisions\"]:\n        return END\n    return \"reflect\"","metadata":{"id":"75c8d7021369","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:51:00.507471Z","iopub.execute_input":"2025-03-03T10:51:00.507833Z","iopub.status.idle":"2025-03-03T10:51:00.518531Z","shell.execute_reply.started":"2025-03-03T10:51:00.507807Z","shell.execute_reply":"2025-03-03T10:51:00.517488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define and compile the graph","metadata":{"id":"48569416595a"}},{"cell_type":"code","source":"# Initialize the state graph\nbuilder = StateGraph(AgentState)\n\n# Add nodes for each step in the workflow\nbuilder.add_node(\"planner\", plan_node)\nbuilder.add_node(\"generate\", generation_node)\nbuilder.add_node(\"reflect\", reflection_node)\nbuilder.add_node(\"research_plan\", research_plan_node)\nbuilder.add_node(\"research_critique\", research_critique_node)\n\n# Set the entry point of the workflow\nbuilder.set_entry_point(\"planner\")\n\n# Add conditional edges for task continuation or end\nbuilder.add_conditional_edges(\n    \"generate\", should_continue, {END: END, \"reflect\": \"reflect\"}\n)\n\n# Define task sequence edges\nbuilder.add_edge(\"planner\", \"research_plan\")\nbuilder.add_edge(\"research_plan\", \"generate\")\n\nbuilder.add_edge(\"reflect\", \"research_critique\")\nbuilder.add_edge(\"research_critique\", \"generate\")\n\n# Compile the graph with memory state management\ngraph = builder.compile(checkpointer=memory)","metadata":{"id":"86567ad87aa2","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:51:04.819668Z","iopub.execute_input":"2025-03-03T10:51:04.820013Z","iopub.status.idle":"2025-03-03T10:51:04.833185Z","shell.execute_reply.started":"2025-03-03T10:51:04.819989Z","shell.execute_reply":"2025-03-03T10:51:04.831919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Show the compiled graph","metadata":{"id":"44d87b0a2052"}},{"cell_type":"code","source":"Image(graph.get_graph().draw_mermaid_png())","metadata":{"id":"9c3170874384","outputId":"9033137e-d968-4dc4-d421-45c15f6b5dac","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:51:08.307861Z","iopub.execute_input":"2025-03-03T10:51:08.308233Z","iopub.status.idle":"2025-03-03T10:51:08.789687Z","shell.execute_reply.started":"2025-03-03T10:51:08.308203Z","shell.execute_reply":"2025-03-03T10:51:08.788232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run the agent - write on!","metadata":{"id":"c95bebc7e74c"}},{"cell_type":"code","source":"# Define the topic of the essay\nESSAY_TOPIC = \"What were the impacts of Hurricane Helene and Hurricane Milton in 2024?\"\n\n# Define a thread configuration with a unique thread ID\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Stream through the graph execution with an initial task and state\nfor s in graph.stream(\n    {\n        \"task\": ESSAY_TOPIC,  # Initial task\n        \"max_revisions\": 2,  # Maximum number of revisions allowed\n        \"revision_number\": 1,  # Current revision number\n        \"content\": [],  # Initial empty content list\n    },\n    thread,\n):\n    step = next(iter(s))\n    display(Markdown(f\"# {step}\"))\n    for key, content in s[step].items():\n        if key == \"revision_number\":\n            display(Markdown(f\"**Revision Number**: {content}\"))\n        elif isinstance(content, list):\n            for c in content:\n                display(Markdown(c))\n        else:\n            display(Markdown(content))\n    print(\"\\n---\\n\")","metadata":{"id":"2a849843b454","outputId":"bf173304-1086-47d1-e920-03f10f0017e1","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:51:11.836586Z","iopub.execute_input":"2025-03-03T10:51:11.83701Z","iopub.status.idle":"2025-03-03T10:52:01.819543Z","shell.execute_reply.started":"2025-03-03T10:51:11.836977Z","shell.execute_reply":"2025-03-03T10:52:01.81848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Output the final draft of the essay","metadata":{"id":"4c95d0cd7f6a"}},{"cell_type":"code","source":"display(Markdown(s[\"generate\"][\"draft\"]))","metadata":{"id":"e50e674081f2","outputId":"594f7134-06ca-4146-f35a-65a4a0490564","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:52:01.820915Z","iopub.execute_input":"2025-03-03T10:52:01.821209Z","iopub.status.idle":"2025-03-03T10:52:01.827081Z","shell.execute_reply.started":"2025-03-03T10:52:01.821185Z","shell.execute_reply":"2025-03-03T10:52:01.826246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Additional Resources\n\n- [Google Cloud Generative AI repository on GitHub](https://github.com/GoogleCloudPlatform/generative-ai/)\n- [Gemini API in Google AI Studio](https://ai.google.dev/gemini-api/docs)\n- [Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview)\n- [LangGraph tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\n- [DeepLearning.AI course on AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/)","metadata":{"id":"c01d26c8df5b"}}]}